{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache.processor.pre import last_content\n",
    "content = last_content({\"messages\": [{\"content\": \"foo1\"}, {\"content\": \"foo2\"}]})\n",
    "# content = \"foo2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animal']\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "from gptcache import Config\n",
    "from gptcache.processor.pre import last_content_without_template\n",
    "\n",
    "template_obj = PromptTemplate.from_template(\"tell me a joke about {subject}\")\n",
    "prompt = template_obj.format(subject=\"animal\")\n",
    "\n",
    "value = last_content_without_template(\n",
    "    data={\"messages\": [{\"content\": prompt}]},\n",
    "    cache_config=Config(template=template_obj.template),\n",
    ")\n",
    "print(value)\n",
    "# ['animal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache.embedding import LangChain\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "test_sentence = 'Hello, world.'\n",
    "#embeddings = OpenAIEmbeddings(model=\"your-embeddings-deployment-name\")\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-12-01-preview\",  # e.g., \"2023-12-01-preview\"\n",
    ")\n",
    "encoder = LangChain(embeddings=embeddings)\n",
    "embed = encoder.to_embeddings(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "# Load config values\n",
    "openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\") \n",
    "azure_deployment=\"gpt-4-32k\"\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\") \n",
    "openai_api_type=\"azure\"\n",
    "\n",
    "# Create an instance of chat llm\n",
    "# client = AzureOpenAI(\n",
    "#         azure_endpoint=openai_api_base,\n",
    "#         openai_api_key=openai_api_key,\n",
    "#         openai_api_version=openai_api_version,\n",
    "#         azure_deployment=azure_deployment,   \n",
    "#     )\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    api_version=openai_api_version,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\sarrabelly\\Applications\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what's github\n",
      "Time consuming: 9.13s\n",
      "Answer: GitHub is a web-based platform used for version control and collaboration. It allows multiple people to work on projects at the same time. It's mainly used by programmers for developing software, but can also be used to manage and store documents. GitHub allows users to make copies of repositories, make changes to these copies and then propose these changes to the original repository (a process called \"pull request\"). In addition, GitHub provides a variety of other features, such as bug tracking, feature request, task management, and more.\n",
      "\n",
      "Question: can you explain what GitHub is\n",
      "Time consuming: 7.67s\n",
      "Answer: GitHub is a web-based platform used for version control and collaboration. It allows multiple people to work on projects concurrently. It was designed around Git, which is an open-source version control software that allows users to download the current version of a project, work on it, and then upload it with a set of updates and changes.\n",
      "\n",
      "GitHub is immensely popular among developers for sharing and storing code. It provides features such as bug tracking, feature requests, task management, and wikis for every project. It basically provides a cloud-based service for developers to store and manage their source code, with the added benefit of tracking and controlling changes to their code.\n",
      "\n",
      "Question: can you tell me more about GitHub\n",
      "Time consuming: 17.14s\n",
      "Answer: GitHub is a web-based hosting service for version control using Git, which is primarily used for computer code. It was founded in 2008 by Tom Preston-Werner, Chris Wanstrath, and PJ Hyett, and is now owned by Microsoft. \n",
      "\n",
      "The platform provides access control and several collaboration features like bug tracking, feature requests, task management, and wikis for every project. It offers both free accounts - which are commonly used to host open-source software projects - and private repositories, which require a paid subscription. It is the largest host of source code in the world, with over 40 million users and more than 100 million repositories (including at least 28 million public repositories) as of 2020.\n",
      "\n",
      "For software development projects, GitHub provides a desktop interface (for macOS and Windows), an integrated issue-tracking system, graphical representation of branches, and a GUI (Graphical User Interface) for Git that includes comprehensive repository statistics. It also provides access to the codebase for public and open-source projects, and any user can \"fork\" these projects to copy and modify the source code. \n",
      "\n",
      "GitHub also introduced features like a web-based graphical interface for managing pull requests, special file formats for \"GitHub Pages\" (websites hosted directly from repositories), and \"GitHub Actions\" (automated user or service responses to repository events like push requests). \n",
      "\n",
      "The site also includes social networking functions such as feeds, followers and a network graph to display how developers work on their versions of a repository.\n",
      "\n",
      "Question: what is the purpose of GitHub\n",
      "Time consuming: 7.32s\n",
      "Answer: GitHub is a platform used to store and share code. Its primary purposes are version control and collaboration, allowing multiple people to work on a project simultaneously without overwriting each other's work. It's essentially a social network for programmers and developers where they can contribute to developing software, track and control changes to projects. It also provides a place for learning, networking, and developing one's skills. Users can share ideas, collaborate on projects and review each other's work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gptcache import Cache, Config\n",
    "#from gptcache.adapter import openai\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "from gptcache.embedding import Onnx\n",
    "from gptcache.manager import manager_factory\n",
    "from gptcache.processor.post import random_one\n",
    "from gptcache.processor.pre import last_content\n",
    "from gptcache.similarity_evaluation import OnnxModelEvaluation\n",
    "\n",
    "openai_complete_cache = Cache()\n",
    "\n",
    "encoder = Onnx()\n",
    "sqlite_faiss_data_manager = manager_factory(\n",
    "    \"sqlite,faiss\",\n",
    "    data_dir=\"openai_complete_cache\",\n",
    "    scalar_params={\n",
    "        \"sql_url\": \"sqlite:///./openai_complete_cache.db\",\n",
    "        \"table_name\": \"openai_chat\",\n",
    "    },\n",
    "    vector_params={\n",
    "        \"dimension\": encoder.dimension,\n",
    "        \"index_file_path\": \"./openai_chat_faiss.index\",\n",
    "    },\n",
    ")\n",
    "onnx_evaluation = OnnxModelEvaluation()\n",
    "cache_config = Config(similarity_threshold=0.75)\n",
    "\n",
    "init_similar_cache(\n",
    "    cache_obj=openai_complete_cache,\n",
    "    pre_func=last_content,\n",
    "    embedding=encoder,\n",
    "    data_manager=sqlite_faiss_data_manager,\n",
    "    evaluation=onnx_evaluation,\n",
    "    post_func=random_one,\n",
    "    config=cache_config,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"what's github\",\n",
    "    \"can you explain what GitHub is\",\n",
    "    \"can you tell me more about GitHub\",\n",
    "    \"what is the purpose of GitHub\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    start_time = time.time()\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    #     cache_obj=openai_complete_cache,\n",
    "    # )\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model=\"gpt-4-32k\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    #     cache_obj=openai_complete_cache,\n",
    "    # )\n",
    "    response = client.chat.completions.create(        \n",
    "        model=\"gpt-4-32k\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        )\n",
    "    # cache_obj[question] = response\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))\n",
    "    # print(f'Answer: {response[\"choices\"][0][\"message\"][\"content\"]}\\n')\n",
    "    print(f'Answer: {response.choices[0].message.content}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Text To Image generation:_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache import cache\n",
    "from gptcache.adapter import openai\n",
    "from gptcache.processor.pre import get_prompt\n",
    "\n",
    "from gptcache.embedding import Onnx\n",
    "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n",
    "from gptcache.manager import get_data_manager, CacheBase, VectorBase, ObjectBase\n",
    "\n",
    "onnx = Onnx()\n",
    "cache_base = CacheBase('sqlite')\n",
    "vector_base = VectorBase('milvus', host='localhost', port='19530', dimension=onnx.dimension)\n",
    "object_base = ObjectBase('local', path='./images')\n",
    "data_manager = get_data_manager(cache_base, vector_base, object_base)\n",
    "\n",
    "cache.init(\n",
    "    pre_embedding_func=get_prompt,\n",
    "    embedding_func=onnx.to_embeddings,\n",
    "    data_manager=data_manager,\n",
    "    similarity_evaluation=SearchDistanceEvaluation(),\n",
    "    )\n",
    "cache.set_openai_key()\n",
    "\n",
    "response = openai.Image.create(\n",
    "  prompt=\"a white siamese cat\",\n",
    "  n=1,\n",
    "  size=\"256x256\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "\n",
    "response = openai.Image.create(\n",
    "  prompt=\"a white siamese cat\",\n",
    "  n=1,\n",
    "  size=\"256x256\"\n",
    ")\n",
    "image_url = response['data'][0]['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NL2SQL / Codex scenarios:_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def response_text(openai_resp):\n",
    "    return openai_resp[\"choices\"][0][\"text\"]\n",
    "\n",
    "from gptcache import cache\n",
    "from gptcache.adapter import openai\n",
    "from gptcache.embedding import Onnx\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.manager import CacheBase, VectorBase, get_data_manager\n",
    "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n",
    "\n",
    "print(\"Cache loading.....\")\n",
    "\n",
    "onnx = Onnx()\n",
    "data_manager = get_data_manager(CacheBase(\"sqlite\"), VectorBase(\"faiss\", dimension=onnx.dimension))\n",
    "cache.init(pre_embedding_func=get_prompt,\n",
    "    embedding_func=onnx.to_embeddings,\n",
    "    data_manager=data_manager,\n",
    "    similarity_evaluation=SearchDistanceEvaluation(),\n",
    "    )\n",
    "cache.set_openai_key()\n",
    "\n",
    "questions = [\n",
    "    \"A query to list the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n",
    "    \"Query the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n",
    "    \"List the names of the departments which employed more than 10 employees in the last 3 months\\nSELECT\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    start_time = time.time()\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"gpt-35-turbo-instruct\",\n",
    "      prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### \" + question,\n",
    "      temperature=0,\n",
    "      max_tokens=150,\n",
    "      top_p=1.0,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "      stop=[\"#\", \";\"]\n",
    "    )\n",
    "    print(question, response_text(response))\n",
    "    print(\"Time consuming: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0rc2\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.2.0rc2Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sarrabelly\\Applications\\.venv\\Lib\\site-packages\\~qlalchemy'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading langchain-0.2.0rc2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (6.0.1)\n",
      "Collecting SQLAlchemy<2.0.29,>=1.4 (from langchain==0.2.0rc2)\n",
      "  Downloading SQLAlchemy-2.0.28-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (0.1.58)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain==0.2.0rc2) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0rc2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0rc2) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0rc2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0rc2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0rc2) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0rc2) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0rc2) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.2.0rc2) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.2.0rc2) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0rc2) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.0rc2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.0rc2) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.0rc2) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.0rc2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.0rc2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.0rc2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.0rc2) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from SQLAlchemy<2.0.29,>=1.4->langchain==0.2.0rc2) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.2.0rc2) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sarrabelly\\applications\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0rc2) (1.0.0)\n",
      "Downloading langchain-0.2.0rc2-py3-none-any.whl (973 kB)\n",
      "   ---------------------------------------- 0.0/973.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 245.8/973.8 kB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 737.3/973.8 kB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 973.8/973.8 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.28-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.1 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/2.1 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.1/2.1 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.1 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: SQLAlchemy, langchain\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.30\n",
      "    Uninstalling SQLAlchemy-2.0.30:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.30\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.20\n",
      "    Uninstalling langchain-0.1.20:\n",
      "      Successfully uninstalled langchain-0.1.20\n",
      "Successfully installed SQLAlchemy-2.0.28 langchain-0.2.0rc2\n"
     ]
    }
   ],
   "source": [
    "pip install langchain==0.2.0rc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
